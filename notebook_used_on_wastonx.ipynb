{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab77d18b",
   "metadata": {},
   "source": [
    "# Models & Training - Drone Fault Detection\n",
    "\n",
    "This notebook consolidates all model definitions and training pipelines:\n",
    "- **Classical Models**: Random Forest for fault detection, type classification, and severity regression\n",
    "- **Deep Learning MTL**: Multi-Task Learning CNN for simultaneous prediction\n",
    "- **Unsupervised Learning**: Isolation Forest for anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. **Setup & Imports**\n",
    "2. **Model Definitions**\n",
    "   - Classical Models (Random Forest)\n",
    "   - Deep MTL (CNN 1D)\n",
    "3. **Training Functions**\n",
    "   - Classical Training\n",
    "   - Deep MTL Training\n",
    "   - Unsupervised Training\n",
    "4. **Evaluation Functions**\n",
    "5. **Training Pipelines** (Main scripts as cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f708aba",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da7c0d",
   "metadata": {
    "id": "3ae9dc00-54dd-430b-9657-7f8d12322417"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "from joblib import dump, load\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae93d7",
   "metadata": {},
   "source": [
    "## 2. Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fa5b9",
   "metadata": {},
   "source": [
    "### 2.1 Classical Models (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454da698",
   "metadata": {
    "id": "4a7af472-1b9b-4942-9bd5-fd2d5e116c0e"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClassicalModels:\n",
    "    \"\"\"Container for classical ML models\"\"\"\n",
    "    fault_clf: RandomForestClassifier\n",
    "    type_clf: RandomForestClassifier\n",
    "    severity_reg: RandomForestRegressor\n",
    "\n",
    "\n",
    "def train_classical_models(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_fault_train: np.ndarray,\n",
    "    y_type_train: np.ndarray,\n",
    "    y_sev_train: np.ndarray,\n",
    ") -> ClassicalModels:\n",
    "    \"\"\"\n",
    "    Train three models: fault detection, fault type, severity.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Feature matrix (samples √ó features)\n",
    "        y_fault_train: Binary fault labels (0/1)\n",
    "        y_type_train: Fault type labels\n",
    "        y_sev_train: Severity levels (0-3)\n",
    "    \n",
    "    Returns:\n",
    "        ClassicalModels containing trained models\n",
    "    \"\"\"\n",
    "    print(\"Training fault detection classifier...\")\n",
    "    fault_clf = RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42)\n",
    "    fault_clf.fit(X_train, y_fault_train)\n",
    "\n",
    "    print(\"Training fault type classifier...\")\n",
    "    type_clf = RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42)\n",
    "    type_clf.fit(X_train, y_type_train)\n",
    "\n",
    "    print(\"Training severity regressor...\")\n",
    "    severity_reg = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "    severity_reg.fit(X_train, y_sev_train)\n",
    "\n",
    "    return ClassicalModels(\n",
    "        fault_clf=fault_clf, type_clf=type_clf, severity_reg=severity_reg\n",
    "    )\n",
    "\n",
    "\n",
    "def predict_with_classical(\n",
    "    models: ClassicalModels, X: pd.DataFrame\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Make predictions with classical models.\n",
    "    \n",
    "    Returns:\n",
    "        prob_fault: Probability of fault (0-1)\n",
    "        type_pred: Predicted fault type\n",
    "        sev_pred: Predicted severity level\n",
    "    \"\"\"\n",
    "    prob_fault = models.fault_clf.predict_proba(X)[:, 1]\n",
    "    type_pred = models.type_clf.predict(X)\n",
    "    sev_pred = models.severity_reg.predict(X)\n",
    "    return prob_fault, type_pred, sev_pred\n",
    "\n",
    "print(\"‚úì Classical models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc85789",
   "metadata": {},
   "source": [
    "### 2.2 Deep Learning Multi-Task Learning (CNN 1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcceb49e",
   "metadata": {
    "id": "818c1961-d0eb-4bba-a690-5020be3f670e"
   },
   "outputs": [],
   "source": [
    "class CNNMTL(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN Multi-Task Learning model for drone fault detection.\n",
    "    \n",
    "    Input: (batch, seq_len, n_features)\n",
    "    Outputs: fault detection, fault type, severity level\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, n_fault_types: int):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.head_fault = nn.Linear(256, 2)  # Binary fault detection\n",
    "        self.head_type = nn.Linear(256, n_fault_types)  # Fault type classification\n",
    "        self.head_sev = nn.Linear(256, 4)  # 4 severity levels (0-3)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # x: (batch, seq_len, n_feat) -> (batch, n_feat, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        h = self.backbone(x).squeeze(-1)\n",
    "        out_fault = self.head_fault(h)\n",
    "        out_type = self.head_type(h)\n",
    "        out_sev = self.head_sev(h)\n",
    "        return out_fault, out_type, out_sev\n",
    "\n",
    "\n",
    "def mtl_loss(\n",
    "    logits_fault,\n",
    "    logits_type,\n",
    "    logits_sev,\n",
    "    y_fault,\n",
    "    y_type,\n",
    "    y_sev,\n",
    "    lambda_fault=1.0,\n",
    "    lambda_type=1.0,\n",
    "    lambda_sev=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-task loss function combining three objectives.\n",
    "    \"\"\"\n",
    "    loss_fault = F.cross_entropy(logits_fault, y_fault)\n",
    "    loss_type = F.cross_entropy(logits_type, y_type)\n",
    "    loss_sev = F.cross_entropy(logits_sev, y_sev)\n",
    "    \n",
    "    total = lambda_fault * loss_fault + lambda_type * loss_type + lambda_sev * loss_sev\n",
    "    return total, {\"fault\": loss_fault.item(), \"type\": loss_type.item(), \"sev\": loss_sev.item()}\n",
    "\n",
    "print(\"‚úì Deep MTL model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370ca30",
   "metadata": {},
   "source": [
    "### 2.3 PyTorch Dataset for Time Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bbf78",
   "metadata": {
    "id": "b1e013ab-d6b8-4ce2-b535-875ce4a878fd"
   },
   "outputs": [],
   "source": [
    "class WindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for time windows + labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y_fault: np.ndarray,\n",
    "        y_type: np.ndarray,\n",
    "        y_sev: np.ndarray,\n",
    "    ):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y_fault = y_fault.astype(np.int64)\n",
    "        self.y_type = y_type.astype(np.int64)\n",
    "        self.y_sev = y_sev.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.X[idx],\n",
    "            self.y_fault[idx],\n",
    "            self.y_type[idx],\n",
    "            self.y_sev[idx],\n",
    "        )\n",
    "\n",
    "print(\"‚úì WindowDataset defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5d7d6",
   "metadata": {},
   "source": [
    "## 3. Training Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef90d3",
   "metadata": {},
   "source": [
    "### 3.1 Unsupervised Learning (Isolation Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5964f39",
   "metadata": {
    "id": "2d02d8c6-f134-4811-987d-bd2fa666459d"
   },
   "outputs": [],
   "source": [
    "def train_isolation_forest(X_healthy: np.ndarray) -> IsolationForest:\n",
    "    \"\"\"\n",
    "    Train unsupervised anomaly detector on healthy windows only.\n",
    "    \n",
    "    Args:\n",
    "        X_healthy: Feature matrix of healthy samples only\n",
    "    \n",
    "    Returns:\n",
    "        Trained IsolationForest model\n",
    "    \"\"\"\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=200,\n",
    "        contamination=0.1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    iso.fit(X_healthy)\n",
    "    return iso\n",
    "\n",
    "\n",
    "def anomaly_score(model: IsolationForest, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate anomaly scores (higher = more anomalous).\n",
    "    \"\"\"\n",
    "    # IsolationForest returns negative anomaly scores\n",
    "    raw = model.score_samples(X)\n",
    "    return -raw\n",
    "\n",
    "print(\"‚úì Unsupervised training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a4b0a",
   "metadata": {},
   "source": [
    "### 3.2 Deep MTL Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c77bc",
   "metadata": {
    "id": "50cbc8a4-e801-49d8-ae43-57e827ab9069"
   },
   "outputs": [],
   "source": [
    "def train_deep_mtl(\n",
    "    X: np.ndarray,\n",
    "    y_fault: np.ndarray,\n",
    "    y_type: np.ndarray,\n",
    "    y_sev: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    val_split: float = 0.2,\n",
    "    lambda_fault: float = 1.0,\n",
    "    lambda_type: float = 1.0,\n",
    "    lambda_sev: float = 1.0,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train Multi-Task Learning CNN model.\n",
    "    \n",
    "    Args:\n",
    "        X: Input windows (n_samples, seq_len, n_features)\n",
    "        y_fault, y_type, y_sev: Labels\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        val_split: Validation split ratio\n",
    "        lambda_*: Loss weights for each task\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and validation metrics\n",
    "    \"\"\"\n",
    "    # Split train/val\n",
    "    idx_train, idx_val = train_test_split(\n",
    "        np.arange(len(X)), test_size=val_split, random_state=42, stratify=y_fault\n",
    "    )\n",
    "    \n",
    "    train_dataset = WindowDataset(\n",
    "        X[idx_train], y_fault[idx_train], y_type[idx_train], y_sev[idx_train]\n",
    "    )\n",
    "    val_dataset = WindowDataset(\n",
    "        X[idx_val], y_fault[idx_val], y_type[idx_val], y_sev[idx_val]\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    n_features = X.shape[2]\n",
    "    n_fault_types = int(y_type.max() + 1)\n",
    "    model = CNNMTL(in_channels=n_features, n_fault_types=n_fault_types)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
    "    \n",
    "    # Training loop\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_epoch = 0.0\n",
    "        \n",
    "        for batch_X, batch_fault, batch_type, batch_sev in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_fault = batch_fault.to(device)\n",
    "            batch_type = batch_type.to(device)\n",
    "            batch_sev = batch_sev.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits_fault, logits_type, logits_sev = model(batch_X)\n",
    "            loss, _ = mtl_loss(\n",
    "                logits_fault, logits_type, logits_sev,\n",
    "                batch_fault, batch_type, batch_sev,\n",
    "                lambda_fault, lambda_type, lambda_sev\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += loss.item() * len(batch_X)\n",
    "        \n",
    "        train_loss_epoch /= len(train_dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_fault, batch_type, batch_sev in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_fault = batch_fault.to(device)\n",
    "                batch_type = batch_type.to(device)\n",
    "                batch_sev = batch_sev.to(device)\n",
    "                \n",
    "                logits_fault, logits_type, logits_sev = model(batch_X)\n",
    "                loss, _ = mtl_loss(\n",
    "                    logits_fault, logits_type, logits_sev,\n",
    "                    batch_fault, batch_type, batch_sev,\n",
    "                    lambda_fault, lambda_type, lambda_sev\n",
    "                )\n",
    "                val_loss_epoch += loss.item() * len(batch_X)\n",
    "        \n",
    "        val_loss_epoch /= len(val_dataset)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss_epoch)\n",
    "        history[\"val_loss\"].append(val_loss_epoch)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss_epoch:.4f}, Val Loss: {val_loss_epoch:.4f}\")\n",
    "    \n",
    "    # Final validation metrics\n",
    "    val_metrics = {\"history\": history, \"final_val_loss\": val_loss_epoch}\n",
    "    \n",
    "    return model, val_metrics\n",
    "\n",
    "print(\"‚úì Deep MTL training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aabbf",
   "metadata": {},
   "source": [
    "## 4. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4e19c",
   "metadata": {
    "id": "4797b13b-3c17-4dc5-9f32-b7d32443d6a3"
   },
   "outputs": [],
   "source": [
    "def evaluate_fault_detection(y_true: np.ndarray, y_scores: np.ndarray, thr: float = 0.5) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate binary fault detection.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels (0/1)\n",
    "        y_scores: Predicted probabilities or scores\n",
    "        thr: Threshold for binary classification\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    y_pred = (y_scores >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"threshold\": thr,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_fault_type(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate multi-class fault type classification.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics including confusion matrix\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"conf_mat\": conf_mat,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_severity(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate severity prediction (regression or ordinal).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of MAE metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Also compute MAE with rounded predictions (for ordinal levels)\n",
    "    y_pred_rounded = np.round(y_pred).clip(0, 3).astype(int)\n",
    "    mae_levels = mean_absolute_error(y_true, y_pred_rounded)\n",
    "    \n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mae_levels\": mae_levels,\n",
    "    }\n",
    "\n",
    "print(\"‚úì Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883516c",
   "metadata": {},
   "source": [
    "## 5. Complete Training Pipelines\n",
    "\n",
    "These cells replicate the main training scripts as executable notebook cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67eb4c6",
   "metadata": {},
   "source": [
    "### 5.1 Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150ba9f-435f-4f33-8736-403ecc7015ed",
   "metadata": {
    "id": "f150ba9f-435f-4f33-8736-403ecc7015ed"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='pHKiyR******************U3dUIXs4',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/identity/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.direct.us-south.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "bucket = 'hackatonuav-donotdelete-pr-ctbzcaijxyh2yh'\n",
    "object_key = 'X_windows.npy'\n",
    "\n",
    "# load data of type \"application/octet-stream\" into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "\n",
    "streaming_body_X = cos_client.get_object(Bucket=bucket, Key=object_key)['Body']\n",
    "streaming_body_YF = cos_client.get_object(Bucket=bucket, Key=\"y_fault.npy\")['Body']\n",
    "streaming_body_YT = cos_client.get_object(Bucket=bucket, Key=\"y_type.npy\")['Body']\n",
    "streaming_body_YS = cos_client.get_object(Bucket=bucket, Key=\"y_sev.npy\")['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf7df5",
   "metadata": {
    "id": "3aeb8d40-1444-4e35-afc6-1759cf2df0ca"
   },
   "outputs": [],
   "source": [
    "# Load windowed data\n",
    "X_windows = np.load(io.BytesIO(streaming_body_X.read()), allow_pickle=True)\n",
    "y_fault = np.load(io.BytesIO(streaming_body_YF.read()), allow_pickle=True)\n",
    "y_type = np.load(io.BytesIO(streaming_body_YT.read()), allow_pickle=True)\n",
    "y_sev = np.load(io.BytesIO(streaming_body_YS.read()), allow_pickle=True)\n",
    "\n",
    "print(f\"\\nüìä Data loaded:\")\n",
    "print(f\"  X_windows: {X_windows.shape}\")\n",
    "print(f\"  y_fault: {y_fault.shape}\")\n",
    "print(f\"  y_type: {y_type.shape}\")\n",
    "print(f\"  y_sev: {y_sev.shape}\")\n",
    "print(f\"\\n  Unique fault values: {np.unique(y_fault)}\")\n",
    "print(f\"  Unique type values: {np.unique(y_type)}\")\n",
    "print(f\"  Unique severity values: {np.unique(y_sev)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b26609",
   "metadata": {},
   "source": [
    "### 5.3 Train Classical Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e72e7",
   "metadata": {
    "id": "0712b847-4367-40e0-96d5-457e2b79f987"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING CLASSICAL MODELS (Random Forest)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_fault_train, y_fault_test, y_type_train, y_type_test, y_sev_train, y_sev_test = train_test_split(\n",
    "    X_feat,\n",
    "    y_fault,\n",
    "    y_type,\n",
    "    y_sev,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_fault,\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "\n",
    "# Train models\n",
    "classical_models = train_classical_models(\n",
    "    X_train, y_fault_train, y_type_train, y_sev_train\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "prob_fault_test, type_pred_test, sev_pred_test = predict_with_classical(classical_models, X_test)\n",
    "\n",
    "# Evaluate\n",
    "fault_metrics = evaluate_fault_detection(y_fault_test, prob_fault_test, thr=0.5)\n",
    "type_metrics = evaluate_fault_type(y_type_test, type_pred_test)\n",
    "sev_metrics = evaluate_severity(y_sev_test, sev_pred_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS - CLASSICAL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n=== Fault Detection (Binary) ===\")\n",
    "print(f\"Accuracy: {fault_metrics['accuracy']:.3f}\")\n",
    "print(f\"F1 Score: {fault_metrics['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n=== Fault Type Classification ===\")\n",
    "print(f\"Accuracy: {type_metrics['accuracy']:.3f}\")\n",
    "print(f\"F1 Macro: {type_metrics['f1_macro']:.3f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(type_metrics['conf_mat'])\n",
    "\n",
    "print(\"\\n=== Severity Prediction ===\")\n",
    "print(f\"MAE (continuous): {sev_metrics['mae']:.3f}\")\n",
    "print(f\"MAE (levels 0-3): {sev_metrics['mae_levels']:.3f}\")\n",
    "\n",
    "# Save model\n",
    "model_path = MODELS_DIR / \"classical_rf_models.joblib\"\n",
    "meta = {\n",
    "    \"models\": classical_models,\n",
    "    \"feature_columns\": list(X_feat.columns),\n",
    "    \"metrics\": {\n",
    "        \"fault\": fault_metrics,\n",
    "        \"type\": type_metrics,\n",
    "        \"severity\": sev_metrics,\n",
    "    }\n",
    "}\n",
    "dump(meta, model_path)\n",
    "print(f\"\\n‚úì Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03eb38",
   "metadata": {},
   "source": [
    "### 5.4 Train Deep Multi-Task Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339adce-b38f-4b44-be08-7d43523dff93",
   "metadata": {
    "id": "1339adce-b38f-4b44-be08-7d43523dff93"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590915b1",
   "metadata": {
    "id": "6174d20d-4e6a-40eb-82dc-864f9624ab6e"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING DEEP MULTI-TASK LEARNING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"lr\": 1e-3,\n",
    "    \"val_split\": 0.2,\n",
    "    \"lambda_fault\": 1.0,\n",
    "    \"lambda_type\": 1.0,\n",
    "    \"lambda_sev\": 1.0,\n",
    "}\n",
    "\n",
    "print(f\"Configuration: {config}\")\n",
    "\n",
    "# Train model\n",
    "model, val_metrics = train_deep_mtl(\n",
    "    X_windows,\n",
    "    y_fault,\n",
    "    y_type,\n",
    "    y_sev,\n",
    "    **config\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final validation loss: {val_metrics['final_val_loss']:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_metrics['history']['train_loss'], label='Train Loss')\n",
    "plt.plot(val_metrics['history']['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History - Deep MTL Model')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "model_path = MODELS_DIR / \"deep_mtl_cnn1d.pth\"\n",
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"n_features\": X_windows.shape[2],\n",
    "    \"n_fault_types\": int(y_type.max() + 1),\n",
    "    \"config\": config,\n",
    "    \"val_metrics\": val_metrics,\n",
    "}\n",
    "torch.save(checkpoint, model_path)\n",
    "print(f\"\\n‚úì Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e77e3",
   "metadata": {},
   "source": [
    "### 5.5 Train Unsupervised Anomaly Detection (Isolation Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147a0da",
   "metadata": {
    "id": "9043ea53-5dd6-46b8-b6f6-0ab8df4977b0"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING UNSUPERVISED ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter healthy samples only\n",
    "healthy_mask = (y_fault == 0)\n",
    "X_healthy = X_feat[healthy_mask]\n",
    "\n",
    "print(f\"Healthy samples: {len(X_healthy)} / {len(X_feat)} ({len(X_healthy)/len(X_feat)*100:.1f}%)\")\n",
    "\n",
    "if len(X_healthy) == 0:\n",
    "    print(\"‚ö†Ô∏è  No healthy samples found! Cannot train Isolation Forest.\")\n",
    "else:\n",
    "    # Train model\n",
    "    iso_model = train_isolation_forest(X_healthy.values)\n",
    "    \n",
    "    # Calculate anomaly scores for all samples\n",
    "    scores = anomaly_score(iso_model, X_feat.values)\n",
    "    \n",
    "    # Determine threshold (95th percentile of healthy scores)\n",
    "    healthy_scores = scores[healthy_mask]\n",
    "    threshold = np.percentile(healthy_scores, 95.0)\n",
    "    \n",
    "    print(f\"\\nAnomaly threshold (95th percentile of healthy): {threshold:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    unsup_metrics = evaluate_fault_detection(y_fault, scores, thr=threshold)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION RESULTS - UNSUPERVISED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Accuracy: {unsup_metrics['accuracy']:.3f}\")\n",
    "    print(f\"F1 Score: {unsup_metrics['f1']:.3f}\")\n",
    "    \n",
    "    # Visualize score distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(healthy_scores, bins=50, alpha=0.7, label='Healthy', edgecolor='black')\n",
    "    plt.hist(scores[~healthy_mask], bins=50, alpha=0.7, label='Faulty', edgecolor='black')\n",
    "    plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Anomaly Score Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(range(len(scores)), scores, c=y_fault, cmap='RdYlGn_r', alpha=0.5, s=10)\n",
    "    plt.axhline(threshold, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.title('Anomaly Scores (colored by true label)')\n",
    "    plt.colorbar(label='Fault (0=healthy, 1=faulty)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"iso_forest_unsupervised.joblib\"\n",
    "    thr_path = MODELS_DIR / \"iso_forest_threshold.npy\"\n",
    "    \n",
    "    dump(iso_model, model_path)\n",
    "    np.save(thr_path, np.array([threshold]))\n",
    "    \n",
    "    print(f\"\\n‚úì Model saved to {model_path}\")\n",
    "    print(f\"‚úì Threshold saved to {thr_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b47ef5",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "### ‚úÖ Models Trained:\n",
    "1. **Classical Random Forest**: 3 separate models for fault detection, type classification, and severity regression\n",
    "2. **Deep MTL CNN**: Single end-to-end model for all three tasks simultaneously\n",
    "3. **Isolation Forest**: Unsupervised anomaly detection\n",
    "\n",
    "### üìÅ Saved Models:\n",
    "- `models/classical_rf_models.joblib`\n",
    "- `models/deep_mtl_cnn1d.pth`\n",
    "- `models/iso_forest_unsupervised.joblib`\n",
    "- `models/iso_forest_threshold.npy`\n",
    "\n",
    "### üéØ Next Steps:\n",
    "1. **Model Comparison**: Compare all three approaches\n",
    "2. **Hyperparameter Tuning**: Optimize model parameters\n",
    "3. **Feature Engineering**: Try additional features\n",
    "4. **Ensemble Methods**: Combine multiple models\n",
    "5. **Deployment**: Integrate best model into production system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
